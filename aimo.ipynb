{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Credits:\n","\n","- https://www.kaggle.com/code/olyatsimboy/aimo-openmath-mistral-baseline\n","- https://www.kaggle.com/code/aatiffraz/prompt-prediction-w-mixtral-mistral7b-gemma-llama\n","- https://www.kaggle.com/code/thedrcat/aimo-mixtral-baseline\n","- https://www.kaggle.com/code/awsaf49/aimo-kerasnlp-starter\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%writefile /tmp/requirements.txt\n","\n","-U ../input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! pip install -r /tmp/requirements.txt -qq\n","print('done')"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T12:07:42.749352Z","iopub.status.busy":"2024-05-19T12:07:42.749041Z","iopub.status.idle":"2024-05-19T12:07:42.754172Z","shell.execute_reply":"2024-05-19T12:07:42.753172Z","shell.execute_reply.started":"2024-05-19T12:07:42.749329Z"},"metadata":{},"trusted":true},"outputs":[],"source":["from aimo_log import logger\n","import postprocess\n","import random\n","from contextlib import contextmanager\n","from transformers import AutoTokenizer, GenerationConfig, AutoModelForCausalLM\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["DEBUG: https://huggingface.co:443 \"HEAD /deepseek-ai/deepseek-math-7b-instruct/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["deepseek_model_name = \"deepseek-ai/deepseek-math-7b-instruct\"\n","deepseek_tokenizer = AutoTokenizer.from_pretrained(deepseek_model_name)\n","mistral_model_name = \"../input/mistral\" # \"/Mistral-7B-Instruct-v0.2\"\n","mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_model_name)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["DEBUG: https://huggingface.co:443 \"HEAD /deepseek-ai/deepseek-math-7b-instruct/resolve/main/generation_config.json HTTP/1.1\" 200 0\n"]}],"source":["deepseek_generation_config = GenerationConfig.from_pretrained(deepseek_model_name)\n","mistral_generation_config = GenerationConfig.from_pretrained(mistral_model_name)"]},{"cell_type":"markdown","metadata":{},"source":["# `GenerationConfig` documentation\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig \n","\n","Class that holds a configuration for a generation task. A `generate` call supports the following generation methods\n","for text-decoder, text-to-text, speech-to-text, and vision-to-text models:\n","\n","- *greedy decoding* if `num_beams=1` and `do_sample=False`\n","\n","- *contrastive search* if `penalty_alpha>0.` and `top_k>1`\n","\n","- *multinomial sampling* if `num_beams=1` and `do_sample=True`\n","\n","- *beam-search decoding* if `num_beams>1` and `do_sample=False`\n","\n","- *beam-search multinomial sampling* if `num_beams>1` and `do_sample=True`\n","\n","- *diverse beam-search decoding* if `num_beams>1` and `num_beam_groups>1`\n","\n","- *constrained beam-search decoding* if `constraints!=None` or `force_words_ids!=None`\n","\n","- *assisted decoding* if `assistant_model` or `prompt_lookup_num_tokens` is passed to `.generate()`\n","\n","To learn more about decoding strategies refer to the [text generation strategies guide](../generation_strategies).\n","\n","\n","> A large number of these flags control the logits or the stopping criteria of the generation. Make sure you check\n","> the [generate-related classes](https://huggingface.co/docs/transformers/internal/generation_utils) for a full\n","> description of the possible manipulations, as well as examples of their usage.\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Arg:\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### Parameters that control the length of the output\n","\n","- `max_length` (`int`, *optional*, defaults to 20):\n","    The maximum length the generated tokens can have. Corresponds to the length of the input prompt +\n","    `max_new_tokens`. Its effect is overridden by `max_new_tokens`, if also set.\n","- `max_new_tokens` (`int`, *optional*):\n","    The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n","- `min_length` (`int`, *optional*, defaults to 0):\n","    The minimum length of the sequence to be generated. Corresponds to the length of the input prompt +\n","    `min_new_tokens`. Its effect is overridden by `min_new_tokens`, if also set.\n","- `min_new_tokens` (`int`, *optional*):\n","    The minimum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n","- `early_stopping` (`bool` or `str`, *optional*, defaults to `False`):\n","    Controls the stopping condition for beam-based methods, like beam-search. It accepts the following values:\n","    `True`, where the generation stops as soon as there are `num_beams` complete candidates; `False`, where an\n","    heuristic is applied and the generation stops when is it very unlikely to find better candidates;\n","    `\"never\"`, where the beam search procedure only stops when there cannot be better candidates (canonical\n","    beam search algorithm).\n","- `max_time` (`float`, *optional*):\n","    The maximum amount of time you allow the computation to run for in seconds. generation will still finish\n","    the current pass after allocated time has been passed.\n","- `stop_strings` (`str or List[str]`, *optional*):\n","    A string or a list of strings that should terminate generation if the model outputs them.\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","###  Parameters that control the generation strategy used\n","\n","- do_sample (`bool`, *optional*, defaults to `False`):\n","    Whether or not to use sampling ; use greedy decoding otherwise.\n","- num_beams (`int`, *optional*, defaults to 1):\n","    Number of beams for beam search. 1 means no beam search.\n","- num_beam_groups (`int`, *optional*, defaults to 1):\n","    Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams.\n","    [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.\n","- penalty_alpha (`float`, *optional*):\n","    The values balance the model confidence and the degeneration penalty in contrastive search decoding.\n","- use_cache (`bool`, *optional*, defaults to `True`):\n","    Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n","    speed up decoding.\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### Parameters for manipulation of the model output logits\n","\n","- temperature (`float`, *optional*, defaults to 1.0):\n","    The value used to modulate the next token probabilities.\n","- top_k (`int`, *optional*, defaults to 50):\n","    The number of highest probability vocabulary tokens to keep for top-k-filtering.\n","- top_p (`float`, *optional*, defaults to 1.0):\n","    If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to\n","    `top_p` or higher are kept for generation.\n","- min_p (`float`, *optional*):\n","    Minimum token probability, which will be scaled by the probability of the most likely token. It must be a\n","    value between 0 and 1. Typical values are in the 0.01-0.2 range, comparably selective as setting `top_p` in\n","    the 0.99-0.8 range (use the opposite of normal `top_p` values).\n","- typical_p (`float`, *optional*, defaults to 1.0):\n","    Local typicality measures how similar the conditional probability of predicting a target token next is to\n","    the expected conditional probability of predicting a random token next, given the partial text already\n","    generated. If set to float < 1, the smallest set of the most locally typical tokens with probabilities that\n","    add up to `typical_p` or higher are kept for generation. See [this\n","    paper](https://arxiv.org/pdf/2202.00666.pdf) for more details.\n","- epsilon_cutoff (`float`, *optional*, defaults to 0.0):\n","    If set to float strictly between 0 and 1, only tokens with a conditional probability greater than\n","    `epsilon_cutoff` will be sampled. In the paper, suggested values range from 3e-4 to 9e-4, depending on the\n","    size of the model. See [Truncation Sampling as Language Model\n","    Desmoothing](https://arxiv.org/abs/2210.15191) for more details.\n","- eta_cutoff (`float`, *optional*, defaults to 0.0):\n","    Eta sampling is a hybrid of locally typical sampling and epsilon sampling. If set to float strictly between\n","    0 and 1, a token is only considered if it is greater than either `eta_cutoff` or `sqrt(eta_cutoff) *\n","    exp(-entropy(softmax(next_token_logits)))`. The latter term is intuitively the expected next token\n","    probability, scaled by `sqrt(eta_cutoff)`. In the paper, suggested values range from 3e-4 to 2e-3,\n","    depending on the size of the model. See [Truncation Sampling as Language Model\n","    Desmoothing](https://arxiv.org/abs/2210.15191) for more details.\n","- diversity_penalty (`float`, *optional*, defaults to 0.0):\n","    This value is subtracted from a beam's score if it generates a token same as any beam from other group at a\n","    particular time. Note that `diversity_penalty` is only effective if `group beam search` is enabled.\n","- repetition_penalty (`float`, *optional*, defaults to 1.0):\n","    The parameter for repetition penalty. 1.0 means no penalty. See [this\n","    paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.\n","- encoder_repetition_penalty (`float`, *optional*, defaults to 1.0):\n","    The paramater for encoder_repetition_penalty. An exponential penalty on sequences that are not in the\n","    original input. 1.0 means no penalty.\n","- length_penalty (`float`, *optional*, defaults to 1.0):\n","    Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent to\n","    the sequence length, which in turn is used to divide the score of the sequence. Since the score is the log\n","    likelihood of the sequence (i.e. negative), `length_penalty` > 0.0 promotes longer sequences, while\n","    `length_penalty` < 0.0 encourages shorter sequences.\n","- no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n","    If set to int > 0, all ngrams of that size can only occur once.\n","- bad_words_ids(`List[List[int]]`, *optional*):\n","    List of list of token ids that are not allowed to be generated. Check\n","    [`~generation.NoBadWordsLogitsProcessor`] for further documentation and examples.\n","- force_words_ids(`List[List[int]]` or `List[List[List[int]]]`, *optional*):\n","    List of token ids that must be generated. If given a `List[List[int]]`, this is treated as a simple list of\n","    words that must be included, the opposite to `bad_words_ids`. If given `List[List[List[int]]]`, this\n","    triggers a [disjunctive constraint](https://github.com/huggingface/transformers/issues/14081), where one\n","    can allow different forms of each word.\n","- renormalize_logits (`bool`, *optional*, defaults to `False`):\n","    Whether to renormalize the logits after applying all the logits processors or warpers (including the custom\n","    ones). It's highly recommended to set this flag to `True` as the search algorithms suppose the score logits\n","    are normalized but some logit processors or warpers break the normalization.\n","- constraints (`List[Constraint]`, *optional*):\n","    Custom constraints that can be added to the generation to ensure that the output will contain the use of\n","    certain tokens as defined by `Constraint` objects, in the most sensible way possible.\n","- forced_bos_token_id (`int`, *optional*, defaults to `model.config.forced_bos_token_id`):\n","    The id of the token to force as the first generated token after the `decoder_start_token_id`. Useful for\n","    multilingual models like [mBART](../model_doc/mbart) where the first generated token needs to be the target\n","    language token.\n","- forced_eos_token_id (`Union[int, List[int]]`, *optional*, defaults to `model.config.forced_eos_token_id`):\n","    The id of the token to force as the last generated token when `max_length` is reached. Optionally, use a\n","    list to set multiple *end-of-sequence* tokens.\n","- remove_invalid_values (`bool`, *optional*, defaults to `model.config.remove_invalid_values`):\n","    Whether to remove possible *nan* and *inf* outputs of the model to prevent the generation method to crash.\n","    Note that using `remove_invalid_values` can slow down generation.\n","- exponential_decay_length_penalty (`tuple(int, float)`, *optional*):\n","    This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been\n","    generated. The tuple shall consist of: `(start_index, decay_factor)` where `start_index` indicates where\n","    penalty starts and `decay_factor` represents the factor of exponential decay\n","- suppress_tokens  (`List[int]`, *optional*):\n","    A list of tokens that will be suppressed at generation. The `SupressTokens` logit processor will set their\n","    log probs to `-inf` so that they are not sampled.\n","- begin_suppress_tokens  (`List[int]`, *optional*):\n","    A list of tokens that will be suppressed at the beginning of the generation. The `SupressBeginTokens` logit\n","    processor will set their log probs to `-inf` so that they are not sampled.\n","- forced_decoder_ids (`List[List[int]]`, *optional*):\n","    A list of pairs of integers which indicates a mapping from generation indices to token indices that will be\n","    forced before sampling. For example, `[[1, 123]]` means the second generated token will always be a token\n","    of index 123.\n","- sequence_bias (`Dict[Tuple[int], float]`, *optional*)):\n","    Dictionary that maps a sequence of tokens to its bias term. Positive biases increase the odds of the\n","    sequence being selected, while negative biases do the opposite. Check\n","    [`~generation.SequenceBiasLogitsProcessor`] for further documentation and examples.\n","- token_healing (`bool`, *optional*, defaults to `False`):\n","    Heal tail tokens of prompts by replacing them with their appropriate extensions.\n","    This enhances the quality of completions for prompts affected by greedy tokenization bias.\n","- guidance_scale (`float`, *optional*):\n","    The guidance scale for classifier free guidance (CFG). CFG is enabled by setting `guidance_scale > 1`.\n","    Higher guidance scale encourages the model to generate samples that are more closely linked to the input\n","    prompt, usually at the expense of poorer quality.\n","- low_memory (`bool`, *optional*):\n","    Switch to sequential beam search and sequential topk for contrastive search to reduce peak memory.\n","    Used with beam search and contrastive search.\n","- watermarking_config (Union[`WatermarkingConfig`, `dict`], *optional*):\n","    Arguments used to watermark the model outputs by adding a small bias to randomly selected set of \"green\" tokens.\n","    If passed as `Dict`, it will be converted to a `WatermarkingConfig` internally.\n","    See [this paper](https://arxiv.org/abs/2306.04634) for more details. Accepts the following keys:\n","    - greenlist_ratio (`float`):\n","        Used for watermarking. The ratio of \"green\" tokens used to the vocabulary size. Defaults to 0.25.\n","    - bias (`float`):\n","        Used with watermarking. The bias added to the selected \"green\" tokens' logits. Defaults to 2.0.\n","    - hashing_key (`int`):\n","        Hahsing key used for watermarking. Defaults to 15485863 (the millionth prime).\n","    - seeding_scheme (`str`):\n","        Algorithm to use for watermarking. Accepts values:\n","            - \"lefthash\" (default): \"green\" tokens selection depend on the last token (Algorithm 2 from the paper)\n","            - \"selfhash\": \"green\" tokens selection depends on the current token itself (Algorithm 3 from the paper)\n","                The downside of this scheme is that it considers all possible next tokens and can be slower than \"lefthash\".\n","    - context_width(`int`):\n","        The context length of previous tokens to use in seeding. Higher context length makes watermarking more robust.\n","\n","### Parameters that define the output variables of generate\n","\n","- num_return_sequences(`int`, *optional*, defaults to 1):\n","    The number of independently computed returned sequences for each element in the batch.\n","- output_attentions (`bool`, *optional*, defaults to `False`):\n","    Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n","    tensors for more details.\n","- output_hidden_states (`bool`, *optional*, defaults to `False`):\n","    Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n","    more details.\n","- output_scores (`bool`, *optional*, defaults to `False`):\n","    Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n","- output_logits (`bool`, *optional*):\n","    Whether or not to return the unprocessed prediction logit scores. See `logits` under returned tensors for\n","    more details.\n","- return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n","    Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n","\n","### Special tokens that can be used at generation time\n","\n","- pad_token_id (`int`, *optional*):\n","    The id of the *padding* token.\n","- bos_token_id (`int`, *optional*):\n","    The id of the *beginning-of-sequence* token.\n","- eos_token_id (`Union[int, List[int]]`, *optional*):\n","  The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n","\n","### Generation parameters exclusive to encoder-decoder models\n","\n","- encoder_no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n","  If set to int > 0, all ngrams of that size that occur in the `encoder_input_ids` cannot occur in the\n","  `decoder_input_ids`.\n","- decoder_start_token_id (`Union[int, List[int]]`, *optional*):\n","    If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token or a list of length\n","    `batch_size`. Indicating a list enables different start ids for each element in the batch\n","    (e.g. multilingual models with different target languages in one batch)\n","\n","### Generation parameters exclusive to assistant generation\n","- num_assistant_tokens (`int`, *optional*, defaults to 5):\n","    Defines the number of _speculative tokens_ that shall be generated by the assistant model before being\n","    checked by the target model at each iteration. Higher values for `num_assistant_tokens` make the generation\n","    more _speculative_ : If the assistant model is performant larger speed-ups can be reached, if the assistant\n","    model requires lots of corrections, lower speed-ups are reached.\n","- num_assistant_tokens_schedule (`str`, *optional*, defaults to `\"heuristic\"`):\n","    Defines the schedule at which max assistant tokens shall be changed during inference.\n","    - `\"heuristic\"`: When all speculative tokens are correct, increase `num_assistant_tokens` by 2 else\n","      reduce by 1. `num_assistant_tokens` value is persistent over multiple generation calls with the same assistant model.\n","    - `\"heuristic_transient\"`: Same as `\"heuristic\"` but `num_assistant_tokens` is reset to its initial value after each generation call.\n","    - `\"constant\"`: `num_assistant_tokens` stays unchanged during generation\n","- prompt_lookup_num_tokens (`int`, *optional*, default to `None`):\n","    The number of tokens to be output as candidate tokens.\n","- max_matching_ngram_size (`int`, *optional*, default to `None`):\n","    The maximum ngram size to be considered for matching in the prompt. Default to 2 if not provided.\n","###  Parameters specific to the caching mechanism:\n","- cache_implementation (`str`, *optional*, default to `None`):\n","    Cache class that should be used when generating.\n","- cache_config (`Union[CacheConfig, dict]`, *optional*, default to `None`):\n","    Arguments used in the key-value cache class can be passed in `cache_config`. Can be passed as a `Dict` and\n","    it will be converted to its repsective `CacheConfig` internally.\n","    Otherwise can be passed as a `CacheConfig` class matching the indicated `cache_implementation`.\n","### Wild card\n","- generation_kwargs:\n","    Additional generation kwargs will be forwarded to the `generate` function of the model. Kwargs that are not\n","    present in `generate`'s signature will be used in the model forward pass."]},{"cell_type":"markdown","metadata":{},"source":["# Code"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["GenerationConfig {\n","  \"do_sample\": true,\n","  \"max_new_tokens\": 2048,\n","  \"num_return_sequences\": 10,\n","  \"renormalize_logits\": true,\n","  \"temperature\": 1.5\n","}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["GenerationConfig(\n","    max_new_tokens=2048,\n","    temperature=1.5,\n","    do_sample=True,\n","    num_return_sequences=10,\n","    renormalize_logits=True,\n","    # top_k=50,\n","    # top_p=0.95,\n","        \n",")"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# vars(mistral_generation_config)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# vars(deepseek_generation_config)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["class Prompt:\n","    def __init__(self, tokenizer, prompt_format, examples=None, tokenize=False, add_examples=0):\n","        self.tokenizer = tokenizer\n","        self.prompt_format = prompt_format\n","        self.examples = examples\n","        self.tokenize = tokenize\n","        self.add_examples = add_examples\n","        \n","    def __call__(self, problem_text):\n","        examples = \"\"\n","        if self.add_examples > 0  and self.examples is not None:\n","            for example in random.choices(self.examples, k=self.add_examples):\n","                question = example['problem']\n","                solution = random.choice(example['solutions'])\n","                examples += f\"Question: {question}\\nAnswer: {solution}\\n\"\n","\n","        prompt = self.prompt_format.format(\n","            examples=examples,\n","            problem=problem_text\n","        )\n","        return self.tokenizer.apply_chat_template([\n","                {\n","                'role': 'user',\n","                'content': prompt\n","                }\n","            ],\n","            tokenize=self.tokenize,\n","            return_tensors='pt',\n","            add_generation_prompt=True\n","        )\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["@contextmanager\n","def tokenized_prompt(prompt):\n","    try:\n","        tokenize = prompt.tokenize\n","        prompt.tokenize = True \n","        yield prompt\n","    finally:\n","        prompt.tokenize = tokenize\n","\n","class MathSolver:\n","\n","    def __init__(self, llm, prompt):\n","        self.llm = llm\n","        self.prompt = prompt\n","\n","    def __call__(self, problem_text):\n","\n","        with tokenized_prompt(self.prompt) as prompt_maker:\n","            prompt = prompt_maker(problem_text)\n","\n","        response = self.llm.generate(prompt.to(self.llm.device))[0]['generated_text']\n","\n","        return postprocess.get_answer(response)\n","        \n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["deepseek_noinstruct_prompt = Prompt(\n","    tokenizer=deepseek_tokenizer, \n","    prompt_format=(\n","        \"{examples}{problem}\\n\" \n","        \"Please reason step by step, and put your final answer within \\\\boxed{{}}.\"\n","    )\n",")\n","\n","deepseek_instruct_prompt = Prompt(\n","    tokenizer=deepseek_tokenizer, \n","    prompt_format=(\n","        \"{examples}{problem}\\n\" \n","        \"Please integrate natural language reasoning with programs to solve the problem above, \" \n","        \"and put your final answer within \\\\boxed{{}}.\"\n","    )\n",")\n","\n","mistral_template = \"\"\"You are great at solving math problems and Python!\n","Please solve the following problem using Python code and step by step natural language reasoning.\n","Put the final answer within \\\\boxed{{}}.\n","\n","{problem}\n","\"\"\"\n","\n","mistral_prompt = Prompt(\n","    tokenizer=mistral_tokenizer, \n","    prompt_format=mistral_template \n",")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<｜begin▁of▁sentence｜>User: What is the sum of 2 and 2?\n","Please integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\boxed{}.\n","\n","Assistant:\n","<｜begin▁of▁sentence｜>User: What is the sum of 2 and 2?\n","Please reason step by step, and put your final answer within \\boxed{}.\n","\n","Assistant:\n","<s>[INST] You are great at solving math problems and Python!\n","Please solve the following problem using Python code and step by step natural language reasoning.\n","Put the final answer within \\boxed{}.\n","\n","What is the sum of 2 and 2?\n"," [/INST]\n"]}],"source":["print(deepseek_instruct_prompt(\"What is the sum of 2 and 2?\"))\n","print(deepseek_noinstruct_prompt(\"What is the sum of 2 and 2?\"))\n","print(mistral_prompt(\"What is the sum of 2 and 2?\"))\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[100000,   5726,     25,   2461,    317,    254,   2555,    280,    207,\n","             17,    285,    207,     17,     30,    185,   7900,  24621,   3892,\n","           4706,  22834,    366,   6600,    276,   8708,    254,   2066,   2330,\n","             11,    285,   1957,    520,   2328,   3510,   2383,    357,  63962,\n","             90,   1424,    185,    185,  77398,     25]])\n","tensor([[100000,   5726,     25,   2461,    317,    254,   2555,    280,    207,\n","             17,    285,    207,     17,     30,    185,   7900,   2806,   3458,\n","            457,   3458,     11,    285,   1957,    520,   2328,   3510,   2383,\n","            357,  63962,     90,   1424,    185,    185,  77398,     25]])\n","tensor([[    1,   733, 16289, 28793,   995,   460,  1598,   438, 22100, 11049,\n","          4418,   304, 21366, 28808,    13, 12069, 12049,   272,  2296,  2700,\n","          1413, 21366,  2696,   304,  3707,   486,  3707,  4229,  3842, 24685,\n","         28723,    13, 16280,   272,  1480,  4372,  2373,   414,  2858,   286,\n","         28751,  2051,    13,    13,  3195,   349,   272,  2648,   302, 28705,\n","         28750,   304, 28705, 28750, 28804,    13,   733, 28748, 16289, 28793]])\n"]}],"source":["for prompt in [deepseek_instruct_prompt, deepseek_noinstruct_prompt, mistral_prompt]:\n","    with tokenized_prompt(prompt) as prompt_maker:\n","        print(prompt_maker(\"What is the sum of 2 and 2?\"))\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO: submission_mode=False\n"]}],"source":["import os\n","submission_mode = bool(os.getenv('KAGGLE_IS_COMPETITION_RERUN'))\n","logger.info(\"submission_mode=%s\", submission_mode)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO: Running in fake mode\n","INFO: temp file: /tmp/tmpz8pkh3su.py\n","INFO: running python3 on \n","```\n","                                     \n","\n","```\n","INFO: output: \n","INFO: temp file /tmp/tmpz8pkh3su.py to be deleted\n"]}],"source":["if submission_mode:\n","    import aimo\n","else:\n","    import aimo_fake as aimo\n","    logger.info(\"Running in fake mode\")\n","\n","env = aimo.make_env()\n","problems = env.iter_test()\n","submit_answer = env.predict"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# import importlib\n","# importlib.reload(aimo)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0;31mSignature:\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDocstring:\u001b[0m\n","Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model.\n","\n","The model class to instantiate is selected based on the `model_type` property of the config object (either\n","passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by\n","falling back to using pattern matching on `pretrained_model_name_or_path`:\n","\n","    - **bart** -- [`BartForCausalLM`] (BART model)\n","    - **bert** -- [`BertLMHeadModel`] (BERT model)\n","    - **bert-generation** -- [`BertGenerationDecoder`] (Bert Generation model)\n","    - **big_bird** -- [`BigBirdForCausalLM`] (BigBird model)\n","    - **bigbird_pegasus** -- [`BigBirdPegasusForCausalLM`] (BigBird-Pegasus model)\n","    - **biogpt** -- [`BioGptForCausalLM`] (BioGpt model)\n","    - **blenderbot** -- [`BlenderbotForCausalLM`] (Blenderbot model)\n","    - **blenderbot-small** -- [`BlenderbotSmallForCausalLM`] (BlenderbotSmall model)\n","    - **bloom** -- [`BloomForCausalLM`] (BLOOM model)\n","    - **camembert** -- [`CamembertForCausalLM`] (CamemBERT model)\n","    - **code_llama** -- [`LlamaForCausalLM`] (CodeLlama model)\n","    - **codegen** -- [`CodeGenForCausalLM`] (CodeGen model)\n","    - **cohere** -- [`CohereForCausalLM`] (Cohere model)\n","    - **cpmant** -- [`CpmAntForCausalLM`] (CPM-Ant model)\n","    - **ctrl** -- [`CTRLLMHeadModel`] (CTRL model)\n","    - **data2vec-text** -- [`Data2VecTextForCausalLM`] (Data2VecText model)\n","    - **dbrx** -- [`DbrxForCausalLM`] (DBRX model)\n","    - **electra** -- [`ElectraForCausalLM`] (ELECTRA model)\n","    - **ernie** -- [`ErnieForCausalLM`] (ERNIE model)\n","    - **falcon** -- [`FalconForCausalLM`] (Falcon model)\n","    - **fuyu** -- [`FuyuForCausalLM`] (Fuyu model)\n","    - **gemma** -- [`GemmaForCausalLM`] (Gemma model)\n","    - **git** -- [`GitForCausalLM`] (GIT model)\n","    - **gpt-sw3** -- [`GPT2LMHeadModel`] (GPT-Sw3 model)\n","    - **gpt2** -- [`GPT2LMHeadModel`] (OpenAI GPT-2 model)\n","    - **gpt_bigcode** -- [`GPTBigCodeForCausalLM`] (GPTBigCode model)\n","    - **gpt_neo** -- [`GPTNeoForCausalLM`] (GPT Neo model)\n","    - **gpt_neox** -- [`GPTNeoXForCausalLM`] (GPT NeoX model)\n","    - **gpt_neox_japanese** -- [`GPTNeoXJapaneseForCausalLM`] (GPT NeoX Japanese model)\n","    - **gptj** -- [`GPTJForCausalLM`] (GPT-J model)\n","    - **jamba** -- [`JambaForCausalLM`] (Jamba model)\n","    - **jetmoe** -- [`JetMoeForCausalLM`] (JetMoe model)\n","    - **llama** -- [`LlamaForCausalLM`] (LLaMA model)\n","    - **mamba** -- [`MambaForCausalLM`] (Mamba model)\n","    - **marian** -- [`MarianForCausalLM`] (Marian model)\n","    - **mbart** -- [`MBartForCausalLM`] (mBART model)\n","    - **mega** -- [`MegaForCausalLM`] (MEGA model)\n","    - **megatron-bert** -- [`MegatronBertForCausalLM`] (Megatron-BERT model)\n","    - **mistral** -- [`MistralForCausalLM`] (Mistral model)\n","    - **mixtral** -- [`MixtralForCausalLM`] (Mixtral model)\n","    - **mpt** -- [`MptForCausalLM`] (MPT model)\n","    - **musicgen** -- [`MusicgenForCausalLM`] (MusicGen model)\n","    - **musicgen_melody** -- [`MusicgenMelodyForCausalLM`] (MusicGen Melody model)\n","    - **mvp** -- [`MvpForCausalLM`] (MVP model)\n","    - **olmo** -- [`OlmoForCausalLM`] (OLMo model)\n","    - **open-llama** -- [`OpenLlamaForCausalLM`] (OpenLlama model)\n","    - **openai-gpt** -- [`OpenAIGPTLMHeadModel`] (OpenAI GPT model)\n","    - **opt** -- [`OPTForCausalLM`] (OPT model)\n","    - **pegasus** -- [`PegasusForCausalLM`] (Pegasus model)\n","    - **persimmon** -- [`PersimmonForCausalLM`] (Persimmon model)\n","    - **phi** -- [`PhiForCausalLM`] (Phi model)\n","    - **phi3** -- [`Phi3ForCausalLM`] (Phi3 model)\n","    - **plbart** -- [`PLBartForCausalLM`] (PLBart model)\n","    - **prophetnet** -- [`ProphetNetForCausalLM`] (ProphetNet model)\n","    - **qdqbert** -- [`QDQBertLMHeadModel`] (QDQBert model)\n","    - **qwen2** -- [`Qwen2ForCausalLM`] (Qwen2 model)\n","    - **qwen2_moe** -- [`Qwen2MoeForCausalLM`] (Qwen2MoE model)\n","    - **recurrent_gemma** -- [`RecurrentGemmaForCausalLM`] (RecurrentGemma model)\n","    - **reformer** -- [`ReformerModelWithLMHead`] (Reformer model)\n","    - **rembert** -- [`RemBertForCausalLM`] (RemBERT model)\n","    - **roberta** -- [`RobertaForCausalLM`] (RoBERTa model)\n","    - **roberta-prelayernorm** -- [`RobertaPreLayerNormForCausalLM`] (RoBERTa-PreLayerNorm model)\n","    - **roc_bert** -- [`RoCBertForCausalLM`] (RoCBert model)\n","    - **roformer** -- [`RoFormerForCausalLM`] (RoFormer model)\n","    - **rwkv** -- [`RwkvForCausalLM`] (RWKV model)\n","    - **speech_to_text_2** -- [`Speech2Text2ForCausalLM`] (Speech2Text2 model)\n","    - **stablelm** -- [`StableLmForCausalLM`] (StableLm model)\n","    - **starcoder2** -- [`Starcoder2ForCausalLM`] (Starcoder2 model)\n","    - **transfo-xl** -- [`TransfoXLLMHeadModel`] (Transformer-XL model)\n","    - **trocr** -- [`TrOCRForCausalLM`] (TrOCR model)\n","    - **whisper** -- [`WhisperForCausalLM`] (Whisper model)\n","    - **xglm** -- [`XGLMForCausalLM`] (XGLM model)\n","    - **xlm** -- [`XLMWithLMHeadModel`] (XLM model)\n","    - **xlm-prophetnet** -- [`XLMProphetNetForCausalLM`] (XLM-ProphetNet model)\n","    - **xlm-roberta** -- [`XLMRobertaForCausalLM`] (XLM-RoBERTa model)\n","    - **xlm-roberta-xl** -- [`XLMRobertaXLForCausalLM`] (XLM-RoBERTa-XL model)\n","    - **xlnet** -- [`XLNetLMHeadModel`] (XLNet model)\n","    - **xmod** -- [`XmodForCausalLM`] (X-MOD model)\n","\n","The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are\n","deactivated). To train the model, you should first set it back in training mode with `model.train()`\n","\n","Args:\n","    pretrained_model_name_or_path (`str` or `os.PathLike`):\n","        Can be either:\n","\n","            - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n","            - A path to a *directory* containing model weights saved using\n","              [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n","            - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n","              this case, `from_tf` should be set to `True` and a configuration object should be provided as\n","              `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n","              PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n","    model_args (additional positional arguments, *optional*):\n","        Will be passed along to the underlying model `__init__()` method.\n","    config ([`PretrainedConfig`], *optional*):\n","        Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n","        be automatically loaded when:\n","\n","            - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n","              model).\n","            - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n","              save directory.\n","            - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n","              configuration JSON file named *config.json* is found in the directory.\n","    state_dict (*Dict[str, torch.Tensor]*, *optional*):\n","        A state dictionary to use instead of a state dictionary loaded from saved weights file.\n","\n","        This option can be used if you want to create a model from a pretrained configuration but load your own\n","        weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\n","        [`~PreTrainedModel.from_pretrained`] is not a simpler option.\n","    cache_dir (`str` or `os.PathLike`, *optional*):\n","        Path to a directory in which a downloaded pretrained model configuration should be cached if the\n","        standard cache should not be used.\n","    from_tf (`bool`, *optional*, defaults to `False`):\n","        Load the model weights from a TensorFlow checkpoint save file (see docstring of\n","        `pretrained_model_name_or_path` argument).\n","    force_download (`bool`, *optional*, defaults to `False`):\n","        Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n","        cached versions if they exist.\n","    resume_download:\n","        Deprecated and ignored. All downloads are now resumed by default when possible.\n","        Will be removed in v5 of Transformers.\n","    proxies (`Dict[str, str]`, *optional*):\n","        A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n","        'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n","    output_loading_info(`bool`, *optional*, defaults to `False`):\n","        Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n","    local_files_only(`bool`, *optional*, defaults to `False`):\n","        Whether or not to only look at local files (e.g., not try downloading the model).\n","    revision (`str`, *optional*, defaults to `\"main\"`):\n","        The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n","        git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n","        identifier allowed by git.\n","    trust_remote_code (`bool`, *optional*, defaults to `False`):\n","        Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n","        should only be set to `True` for repositories you trust and in which you have read the code, as it will\n","        execute code present on the Hub on your local machine.\n","    code_revision (`str`, *optional*, defaults to `\"main\"`):\n","        The specific revision to use for the code on the Hub, if the code leaves in a different repository than\n","        the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based\n","        system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier\n","        allowed by git.\n","    kwargs (additional keyword arguments, *optional*):\n","        Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n","        `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n","        automatically loaded:\n","\n","            - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n","              underlying model's `__init__` method (we assume all relevant updates to the configuration have\n","              already been done)\n","            - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n","              initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n","              corresponds to a configuration attribute will be used to override said attribute with the\n","              supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n","              will be passed to the underlying model's `__init__` function.\n","\n","Examples:\n","\n","```python\n",">>> from transformers import AutoConfig, AutoModelForCausalLM\n","\n",">>> # Download model and configuration from huggingface.co and cache.\n",">>> model = AutoModelForCausalLM.from_pretrained(\"google-bert/bert-base-cased\")\n","\n",">>> # Update configuration during loading\n",">>> model = AutoModelForCausalLM.from_pretrained(\"google-bert/bert-base-cased\", output_attentions=True)\n",">>> model.config.output_attentions\n","True\n","\n",">>> # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n",">>> config = AutoConfig.from_pretrained(\"./tf_model/bert_tf_model_config.json\")\n",">>> model = AutoModelForCausalLM.from_pretrained(\n","...     \"./tf_model/bert_tf_checkpoint.ckpt.index\", from_tf=True, config=config\n","... )\n","```\n","\u001b[0;31mFile:\u001b[0m      ~/.config/jupyterlab-desktop/jlab_server/envs/python3.10/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\n","\u001b[0;31mType:\u001b[0m      method\n"]}],"source":["AutoModelForCausalLM.from_pretrained?"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["DEBUG: https://huggingface.co:443 \"HEAD /deepseek-ai/deepseek-math-7b-instruct/resolve/main/config.json HTTP/1.1\" 200 0\n","DEBUG: Attempting to acquire lock 139737819215280 on /home/prokaj/.cache/huggingface/hub/.locks/models--deepseek-ai--deepseek-math-7b-instruct/c7ec540a0bea43211102776010779525e60a36d0.lock\n","DEBUG: Lock 139737819215280 acquired on /home/prokaj/.cache/huggingface/hub/.locks/models--deepseek-ai--deepseek-math-7b-instruct/c7ec540a0bea43211102776010779525e60a36d0.lock\n","DEBUG: https://huggingface.co:443 \"GET /deepseek-ai/deepseek-math-7b-instruct/resolve/main/config.json HTTP/1.1\" 200 594\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85bdfc8f6735418288d28ed1f25bcba5","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/594 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["DEBUG: Attempting to release lock 139737819215280 on /home/prokaj/.cache/huggingface/hub/.locks/models--deepseek-ai--deepseek-math-7b-instruct/c7ec540a0bea43211102776010779525e60a36d0.lock\n","DEBUG: Lock 139737819215280 released on /home/prokaj/.cache/huggingface/hub/.locks/models--deepseek-ai--deepseek-math-7b-instruct/c7ec540a0bea43211102776010779525e60a36d0.lock\n","DEBUG: https://huggingface.co:443 \"HEAD /deepseek-ai/deepseek-math-7b-instruct/resolve/main/adapter_config.json HTTP/1.1\" 404 0\n","DEBUG: https://huggingface.co:443 \"HEAD /deepseek-ai/deepseek-math-7b-instruct/resolve/main/model.safetensors HTTP/1.1\" 404 0\n","DEBUG: https://huggingface.co:443 \"HEAD /deepseek-ai/deepseek-math-7b-instruct/resolve/main/model.safetensors.index.json HTTP/1.1\" 404 0\n","DEBUG: https://huggingface.co:443 \"HEAD /deepseek-ai/deepseek-math-7b-instruct/resolve/main/pytorch_model.bin HTTP/1.1\" 404 0\n","DEBUG: https://huggingface.co:443 \"HEAD /deepseek-ai/deepseek-math-7b-instruct/resolve/main/pytorch_model.bin.index.json HTTP/1.1\" 200 0\n","DEBUG: Attempting to acquire lock 139742604067456 on /home/prokaj/.cache/huggingface/hub/.locks/models--deepseek-ai--deepseek-math-7b-instruct/ce6094ab4c40400eca5963470784235f52b7e843.lock\n","DEBUG: Lock 139742604067456 acquired on /home/prokaj/.cache/huggingface/hub/.locks/models--deepseek-ai--deepseek-math-7b-instruct/ce6094ab4c40400eca5963470784235f52b7e843.lock\n","DEBUG: https://huggingface.co:443 \"GET /deepseek-ai/deepseek-math-7b-instruct/resolve/main/pytorch_model.bin.index.json HTTP/1.1\" 200 22464\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ce6f301254b44399e4727b1eb6ba5c7","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin.index.json:   0%|          | 0.00/22.5k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["DEBUG: Attempting to release lock 139742604067456 on /home/prokaj/.cache/huggingface/hub/.locks/models--deepseek-ai--deepseek-math-7b-instruct/ce6094ab4c40400eca5963470784235f52b7e843.lock\n","DEBUG: Lock 139742604067456 released on /home/prokaj/.cache/huggingface/hub/.locks/models--deepseek-ai--deepseek-math-7b-instruct/ce6094ab4c40400eca5963470784235f52b7e843.lock\n","DEBUG: Starting new HTTPS connection (1): huggingface.co:443\n","DEBUG: https://huggingface.co:443 \"HEAD /deepseek-ai/deepseek-math-7b-instruct/resolve/main/model.safetensors.index.json HTTP/1.1\" 404 0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c27e85ddfde045028c4363d732ea89e5","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["DEBUG: https://huggingface.co:443 \"HEAD /deepseek-ai/deepseek-math-7b-instruct/resolve/main/pytorch_model-00001-of-00002.bin HTTP/1.1\" 302 0\n","DEBUG: Attempting to acquire lock 139736860022608 on /home/prokaj/.cache/huggingface/hub/.locks/models--deepseek-ai--deepseek-math-7b-instruct/39c17130070f8eaacb24720d135ef4855540287df8d6ac08a6e46604739a1834.lock\n","DEBUG: Lock 139736860022608 acquired on /home/prokaj/.cache/huggingface/hub/.locks/models--deepseek-ai--deepseek-math-7b-instruct/39c17130070f8eaacb24720d135ef4855540287df8d6ac08a6e46604739a1834.lock\n","DEBUG: Starting new HTTPS connection (1): cdn-lfs-us-1.huggingface.co:443\n","DEBUG: https://cdn-lfs-us-1.huggingface.co:443 \"GET /repos/d1/7c/d17cc38082aa0979c0ea99efec8e7895f2fea88fcd0a946e238974f63c315576/39c17130070f8eaacb24720d135ef4855540287df8d6ac08a6e46604739a1834?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00002.bin%3B+filename%3D%22pytorch_model-00001-of-00002.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1717736397&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNzczNjM5N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2QxLzdjL2QxN2NjMzgwODJhYTA5NzljMGVhOTllZmVjOGU3ODk1ZjJmZWE4OGZjZDBhOTQ2ZTIzODk3NGY2M2MzMTU1NzYvMzljMTcxMzAwNzBmOGVhYWNiMjQ3MjBkMTM1ZWY0ODU1NTQwMjg3ZGY4ZDZhYzA4YTZlNDY2MDQ3MzlhMTgzND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=HL93gIHOWmdMLQcg577CRuo6vvgjF5odGIWSCv4EQdn3UIJhXSkI7HS1cZKGWyOcIOgVTpkPTT~g2qAU3YUZwTE0f~ppmZm3XpxIPFXAo9F5MKOlACjqn4jtIq48VZtwIvQ2n9OmQ0U9PLBelDWnywcJHj45a4r3jRoSEwbX3znNSTb8XCTSxQZCUwZOUpNiXErT0c1GYmBf6WJdGw~INP8~Qy-ld80D1brKnOvGsiRcnSBq42~s4tD5P1Bj7o18vi8aD4ULRc~rhou82ScD6cVrB~C-VQmgfiYHoqvndrDEU2Cp~N0lr8~41Y~q2NOPb0Kl1b2qruPyycfTrfGuqQ__&Key-Pair-Id=KCD77M1F0VK2B HTTP/1.1\" 200 9968194327\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8efad6cc61ad4f948f7f646a0ccc39c8","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.97G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["DEBUG: Attempting to release lock 139736860022608 on /home/prokaj/.cache/huggingface/hub/.locks/models--deepseek-ai--deepseek-math-7b-instruct/39c17130070f8eaacb24720d135ef4855540287df8d6ac08a6e46604739a1834.lock\n","DEBUG: Lock 139736860022608 released on /home/prokaj/.cache/huggingface/hub/.locks/models--deepseek-ai--deepseek-math-7b-instruct/39c17130070f8eaacb24720d135ef4855540287df8d6ac08a6e46604739a1834.lock\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_622598/2221917305.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdeepseek_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepseek_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmath_solver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMathSolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepseek_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepseek_instruct_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/envs/python3.10/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             )\n","\u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/envs/python3.10/lib/python3.10/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_sharded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3510\u001b[0m             \u001b[0;31m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3511\u001b[0;31m             resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n\u001b[0m\u001b[1;32m   3512\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3513\u001b[0m                 \u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/envs/python3.10/lib/python3.10/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;31m# Load from URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m             cached_filename = cached_file(\n\u001b[0m\u001b[1;32m   1041\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mshard_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/envs/python3.10/lib/python3.10/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    400\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/envs/python3.10/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/envs/python3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1219\u001b[0m         )\n\u001b[1;32m   1220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1222\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/envs/python3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mWeakFileLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         _download_to_tmp_and_move(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             \u001b[0mincomplete_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".incomplete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m             \u001b[0mdestination_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/envs/python3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1882\u001b[0m             \u001b[0m_check_disk_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1884\u001b[0;31m         http_get(\n\u001b[0m\u001b[1;32m   1885\u001b[0m             \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1886\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/envs/python3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m                 \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m                 \u001b[0mnew_resume_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m                 \u001b[0;31m# Some data has been downloaded from the server so we reset the number of retries.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["deepseek_model = AutoModelForCausalLM.from_pretrained(\n","    deepseek_model_name,\n","    \n","    \n",")\n","math_solver = MathSolver(llm=deepseek_model, prompt=deepseek_instruct_prompt)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO: Q: What is 1 + 1?\n","INFO: submission:\n","---\n","    answer  true answer\n","id                     \n","0        0            2\n","---\n"]}],"source":["for test, submission in problems:\n","    logger.info(\"Q: %s\", test[\"problem\"].values[0])\n","    answer= math_solver(test['problem'])\n","    submission['answer'] = answer\n","    submit_answer(submission)\n","    logger.info(\"submission:\\n---\\n%s\\n---\", submission)\n","    break"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["options = {\n","    \"model_name\": \"/kaggle/input/deepseek-math\",\n","  \n","}\n","\n","temperature = 0.85\n","do_test = True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T12:07:50.991672Z","iopub.status.busy":"2024-05-19T12:07:50.991333Z","iopub.status.idle":"2024-05-19T12:08:11.096113Z","shell.execute_reply":"2024-05-19T12:08:11.095327Z","shell.execute_reply.started":"2024-05-19T12:07:50.991644Z"},"trusted":true},"outputs":[],"source":["import torch\n","from transformers import (\n","    AutoModelForCausalLM, \n","    AutoTokenizer, \n","    BitsAndBytesConfig, \n","    AutoConfig,\n","    set_seed,\n","    pipeline as transformer_pipeline,\n","    __version__ as transformers_version\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T12:08:11.098126Z","iopub.status.busy":"2024-05-19T12:08:11.097577Z","iopub.status.idle":"2024-05-19T12:08:11.103153Z","shell.execute_reply":"2024-05-19T12:08:11.101959Z","shell.execute_reply.started":"2024-05-19T12:08:11.098100Z"},"trusted":true},"outputs":[],"source":["print(f\"Transformers Version: {transformers_version}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T11:30:55.300700Z","iopub.status.busy":"2024-05-19T11:30:55.300301Z","iopub.status.idle":"2024-05-19T11:30:55.612804Z","shell.execute_reply":"2024-05-19T11:30:55.611723Z","shell.execute_reply.started":"2024-05-19T11:30:55.300669Z"},"trusted":true},"outputs":[],"source":["MODEL_PATH = \"/kaggle/input/deepseek-math\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T11:32:15.364739Z","iopub.status.busy":"2024-05-19T11:32:15.364356Z","iopub.status.idle":"2024-05-19T11:32:15.370316Z","shell.execute_reply":"2024-05-19T11:32:15.369063Z","shell.execute_reply.started":"2024-05-19T11:32:15.364710Z"},"trusted":true},"outputs":[],"source":["print(tokenizer.chat_template)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T12:08:22.169911Z","iopub.status.busy":"2024-05-19T12:08:22.169302Z","iopub.status.idle":"2024-05-19T12:10:46.381213Z","shell.execute_reply":"2024-05-19T12:10:46.380415Z","shell.execute_reply.started":"2024-05-19T12:08:22.169865Z"},"papermill":{"duration":664.688061,"end_time":"2024-02-29T09:36:29.988515","exception":false,"start_time":"2024-02-29T09:25:25.300454","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["set_seed(42)\n","\n","MODEL_PATH = \"/kaggle/input/deepseek-math\"\n","\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit = True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    bnb_4bit_use_double_quant=True,\n",")\n","\n","config = AutoConfig.from_pretrained(MODEL_PATH)\n","config.gradient_checkpointing = True\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n","\n","base_LLM = AutoModelForCausalLM.from_pretrained(\n","    MODEL_PATH,\n","    device_map=\"auto\",\n","    torch_dtype=\"auto\",\n","    trust_remote_code=True,\n","#     quantization_config=quantization_config,\n","    config=config\n",")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T12:16:42.413134Z","iopub.status.busy":"2024-05-19T12:16:42.412756Z","iopub.status.idle":"2024-05-19T12:16:42.422220Z","shell.execute_reply":"2024-05-19T12:16:42.421269Z","shell.execute_reply.started":"2024-05-19T12:16:42.413104Z"},"trusted":true},"outputs":[],"source":["pipeline = transformer_pipeline(\n","    \"text-generation\",\n","    model=base_LLM,\n","    tokenizer=tokenizer,\n","    torch_dtype='auto',\n","    device_map=\"auto\",\n",")\n","\n","pipeline = functools.partial(\n","    pipeline,\n","    max_new_tokens=2048, \n","    do_sample=True, \n","    temperature=temperature,\n","    return_full_text=False\n",")\n","\n","class ExtPipeline:\n","    def __init__(self, pipeline, tokenizer, message_template):\n","        self.pipeline = pipeline\n","        self.tokenizer = tokenizer\n","        self.msg_template = message_template\n","    \n","    \n","    def prompt(self, question):\n","        return self.tokenizer.apply_chat_template(\n","            [\n","                {\n","                    'role': 'user',\n","                    'content': self.msg_template.format(problem=question)\n","                }\n","            ],\n","            tokenize = False\n","        )\n","        \n","        \n","    def __call__(self, question):\n","        \n","        prompt = self.prompt(question)\n","        logger.info('prompt:\\n---\\n%s\\n---', prompt)\n","        \n","        result = self.pipeline(prompt)[0]['generated_text']\n","        \n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","        return result\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T12:16:43.607177Z","iopub.status.busy":"2024-05-19T12:16:43.606820Z","iopub.status.idle":"2024-05-19T12:16:53.671804Z","shell.execute_reply":"2024-05-19T12:16:53.670846Z","shell.execute_reply.started":"2024-05-19T12:16:43.607151Z"},"trusted":true},"outputs":[],"source":["pipe = ExtPipeline(pipeline, tokenizer, deepseek_template)\n","raw_output = pipe(random_equation_question()[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T12:21:31.760862Z","iopub.status.busy":"2024-05-19T12:21:31.759990Z","iopub.status.idle":"2024-05-19T12:21:31.766006Z","shell.execute_reply":"2024-05-19T12:21:31.764977Z","shell.execute_reply.started":"2024-05-19T12:21:31.760818Z"},"trusted":true},"outputs":[],"source":["#raw_output = pipe(random_arithmetic_question()[0])\n","print(raw_output)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T12:26:13.457698Z","iopub.status.busy":"2024-05-19T12:26:13.456841Z","iopub.status.idle":"2024-05-19T12:26:13.463476Z","shell.execute_reply":"2024-05-19T12:26:13.462534Z","shell.execute_reply.started":"2024-05-19T12:26:13.457666Z"},"trusted":true},"outputs":[],"source":["code_chunk(raw_output), get_answer(raw_output)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T12:29:36.496404Z","iopub.status.busy":"2024-05-19T12:29:36.495580Z","iopub.status.idle":"2024-05-19T12:29:36.501322Z","shell.execute_reply":"2024-05-19T12:29:36.500301Z","shell.execute_reply.started":"2024-05-19T12:29:36.496368Z"},"trusted":true},"outputs":[],"source":["chunks = re.split(r\"(?=[\\n\\s]*?)```+?[a-zA-Z\\s]*?(?=\\n)\", raw_output)\n","print(chunks)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T11:12:17.749999Z","iopub.status.busy":"2024-05-19T11:12:17.749634Z","iopub.status.idle":"2024-05-19T11:12:17.756556Z","shell.execute_reply":"2024-05-19T11:12:17.755613Z","shell.execute_reply.started":"2024-05-19T11:12:17.749970Z"},"trusted":true},"outputs":[],"source":["base_LLM.dtype"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T11:12:23.162681Z","iopub.status.busy":"2024-05-19T11:12:23.161802Z","iopub.status.idle":"2024-05-19T11:12:23.166321Z","shell.execute_reply":"2024-05-19T11:12:23.165435Z","shell.execute_reply.started":"2024-05-19T11:12:23.162645Z"},"papermill":{"duration":0.022605,"end_time":"2024-02-29T09:36:31.265878","exception":false,"start_time":"2024-02-29T09:36:31.243273","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["device = 'cuda'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T12:12:02.767421Z","iopub.status.busy":"2024-05-19T12:12:02.767048Z","iopub.status.idle":"2024-05-19T12:12:02.771965Z","shell.execute_reply":"2024-05-19T12:12:02.771035Z","shell.execute_reply.started":"2024-05-19T12:12:02.767393Z"},"trusted":true},"outputs":[],"source":["torch.backends.cuda.enable_mem_efficient_sdp(False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T12:11:23.272568Z","iopub.status.busy":"2024-05-19T12:11:23.271882Z","iopub.status.idle":"2024-05-19T12:11:23.282200Z","shell.execute_reply":"2024-05-19T12:11:23.281303Z","shell.execute_reply.started":"2024-05-19T12:11:23.272537Z"},"trusted":true},"outputs":[],"source":["keras_team_template = \"\"\"Role:\n","You are an advanced AI system with exceptional mathematical reasoning and problem-solving capabilities, specifically designed to solve tricky math problems (whose answer is a non-negative integer) written in LaTeX format from the AI Mathematical Olympiad (AIMO) competition. Your task is to accurately analyze and solve intricate mathematical problems, demonstrating a deep understanding of mathematical concepts and a strong ability to apply logical reasoning strategies.\n","\n","Instruction:\n","1. Carefully read and comprehend the problem statement provided in the \"Problem\" section.\n","2. In the \"Solution\" section, provide a solution of the problem with detailed explanation of your logical reasoning process. Keep in mind that answer must be a non-negative integer number.\n","3. At the end, create a \"Answer\" section where you will state only the final numerical or algebraic answer, without any additional text or narrative.\n","\n","Problem:\n","{problem}\n","\n","Solution:\n","{solution}\n","\"\"\"\n","\n","deepseek_template = \"\"\"Problem:\n","{problem}\n","\n","Instructions:\n","Please integrate natural language reasoning with programs to solve the problem above, \n","and put your final into an \"Answer\" section where you will state only the final numerical \n","or algebraic answer, without any additional text or narrative.'\n","\"\"\"\n","\n","def colorize_text(text):\n","    for word, color in zip([\"Role\", \"Instruction\", \"Problem\", \"Solution\", \"Answer\"],\n","                           [\"blue\", \"orange\", \"red\", \"brown\", \"green\"]):\n","        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n","    return text\n","\n","def is_integer(text):\n","    try:\n","        if int(text) >= 0:\n","            return True\n","        else:\n","            return False\n","    except ValueError:\n","        return False\n","    \n","\n","# Extract answer from model response\n","def get_answer(text):\n","    try:\n","        answer = re.search(r'Answer:\\s*([\\s\\S]+)', text).group(1).strip()\n","        answer = answer.replace(\",\", \"\")\n","        if is_integer(answer):\n","            return int(answer)%1000\n","        else:\n","            return 0\n","    except:\n","        return 0\n","    \n","    \n","def infer(problem, model, template):\n","    # Generate Prompt using template\n","    prompt = template.format(\n","            problem=problem,\n","            solution=\"\"\n","        )\n","\n","    # Infer\n","    return model(prompt) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T12:20:26.997835Z","iopub.status.busy":"2024-05-19T12:20:26.997108Z","iopub.status.idle":"2024-05-19T12:20:27.007082Z","shell.execute_reply":"2024-05-19T12:20:27.006177Z","shell.execute_reply.started":"2024-05-19T12:20:26.997805Z"},"trusted":true},"outputs":[],"source":["@contextlib.contextmanager\n","def new_tempfile():\n","    try:\n","        fd, fname = tempfile.mkstemp(suffix=\".py\")\n","        os.close(fd)\n","        logger.info('temp file: %s', fname)\n","        yield fname\n","    finally:\n","        if os.path.exists(fname):\n","            os.unlink(fname)\n","            logger.info('temp file %s to be deleted', fname)\n","\n","            \n","def run_code_chunk(chunk, timeout=7):\n","    with new_tempfile() as code_file: \n","        with open(code_file, \"w\") as f:\n","            f.write(chunk)\n","\n","        logger.info(\"running python3 on \\n```\\n%s\\n```\", chunk)\n","\n","        try:\n","            result = subprocess.run(\n","                [\"python3\", code_file], \n","                timeout=timeout, \n","                capture_output=True\n","            )\n","        except subprocess.TimeoutExpired:\n","            logger.info(\"timeout occured\")\n","            return -1\n","\n","        if result.returncode:\n","            logger.info(\"error occured %s\", result.stderr.decode('utf8'))\n","            return -1\n","\n","        stdout = result.stdout.decode('utf8')\n","        logger.info(\"output: %s\", stdout)\n","\n","        try:\n","            answer = int(stdout)\n","        except ValueError:\n","            answer = -1\n","\n","        return answer\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T06:30:34.432392Z","iopub.status.busy":"2024-05-19T06:30:34.431988Z","iopub.status.idle":"2024-05-19T06:30:37.895356Z","shell.execute_reply":"2024-05-19T06:30:37.894159Z","shell.execute_reply.started":"2024-05-19T06:30:34.432351Z"},"trusted":true},"outputs":[],"source":["if do_test:\n","    inf_loop = \"\"\"while True:\n","        pass\n","    \"\"\"\n","    ! ls /tmp\n","    result = run_code_chunk(inf_loop, 1)\n","    print(result)\n","    \n","    result = run_code_chunk(code_chunk(example))\n","    print(result)\n","    \n","    chunk = textwrap.dedent(\"\"\"\n","    def f(a, b):\n","        return a*b\n","    \n","    print(f(2, 12))\n","    \"\"\")\n","    result = run_code_chunk(chunk)\n","    print(result)\n","    ! ls /tmp\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# def process_output(output):\n","#     result = output\n","    \n","#     try:\n","#         code = output.split('```')[1][7:]\n","\n","#         with open('code.py', 'w') as fout:\n","#             fout.write(code)\n","\n","#         batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n","#         try:\n","#             shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n","#             print(shell_output)\n","#             code_output = round(float(eval(shell_output))) % 1000\n","#         except:\n","#             code_output = -1\n","\n","#         print('CODE RESULTS', code_output)\n","    \n","#     except Exception as e:\n","#         print(e)\n","#         print('ERROR PARSING')\n","#         code_output = -1\n","    \n","#     try:\n","#         result_output = re.findall(r'\\\\boxed\\{(.*)\\}', result)\n","\n","#         print('BOXED', result_output)\n","#         if not len(result_output):\n","#             result_output = naive_parse(result)\n","#         else:\n","#             result_output = result_output[-1]\n","\n","#         print('BOXED', result_output)\n","#         if not len(result_output):\n","#             result_output = -1\n","        \n","#         else:\n","#             result_output = round(float(eval(result_output))) % 1000\n","    \n","#     except Exception as e:\n","#         print(e)\n","#         print('ERROR PARSING')\n","#         result_output = -1\n","    \n","#     return result_output, code_output"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"papermill":{"duration":34.259365,"end_time":"2024-02-29T09:37:05.548829","exception":false,"start_time":"2024-02-29T09:36:31.289464","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"source":["import re\n","from collections import defaultdict\n","\n","\n","tool_instruction = \" The answer should be given as a non-negative modulo 1000.\"\n","tool_instruction += '\\nPlease integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}.'\n","\n","\n","n_repetitions = 8 if PRIVATE else 2\n","temperature = 0.8964\n","\n","total_results = []\n","total_answers = []\n","\n","\n","for i in tqdm(range(len(df))):\n","    id_ = df['id'].loc[i]\n","    problem = df['problem'].loc[i]\n","\n","    messages = [\n","        {\n","            \"role\": \"user\", \n","            \"content\": problem + tool_instruction\n","        }\n","    ]\n","    \n","    query_prompt = tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=False\n","    )\n","    \n","    results = []\n","    answers = []\n","     \n","    \n","    for _ in tqdm(range(n_repetitions)):\n","        try:\n","            raw_output = pipeline(\n","                query_prompt, \n","                max_new_tokens=2048, \n","                do_sample=True, \n","                temperature=temperature,\n","                return_full_text=False\n","            )\n","            raw_output = raw_output[0]['generated_text']\n","\n","            result_output, code_output = process_output(raw_output)\n","\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","\n","        except Exception as e:\n","            print(e)\n","            result_output, code_output = -1, -1\n","        \n","        results.append(result_output)\n","        answers.append(code_output)\n","    \n","    total_results.append(results)\n","    total_answers.append(answers)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","from collections import Counter\n","\n","df['leng'] = df['problem'].astype(str).map(len)\n","df['orig_index'] = df.index.values\n","df = df.sort_values(by=['leng', 'id']).reset_index(drop=True)\n","df['enumerates'] = range(0, len(df))\n","df = df.sort_values('orig_index').reset_index(drop=True)\n","\n","enumerate_i = 0\n","final_answers = []\n","for a, b in zip(total_answers, total_results):\n","    a = np.array(a)\n","    b = np.array(b)\n","    a[a < 0] = b[a < 0]\n","    pred = Counter(a.tolist()).most_common(2)\n","    pred = pred + [(-1,0)]\n","    val_previously, freq_previously = pred[0]\n","    for val, freq in pred[1:]: \n","        if freq == freq_previously:\n","            val_previously = min(val_previously,val )\n","    enumerates = df.enumerates.values[enumerate_i]\n","    ans = val_previously if not val_previously < 0 else pred[1][0]\n","    enumerate_i+= 1    \n","    final_answers.append(ans)\n","    print(ans)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T06:30:59.649303Z","iopub.status.busy":"2024-05-19T06:30:59.648515Z","iopub.status.idle":"2024-05-19T06:30:59.658726Z","shell.execute_reply":"2024-05-19T06:30:59.657057Z","shell.execute_reply.started":"2024-05-19T06:30:59.649264Z"},"trusted":true},"outputs":[],"source":["class ProblemSolver:\n","    def __init__(\n","        self, \n","        pipeline, \n","        repeat=1,             \n","    ):\n","        self.pipeline = pipeline\n","        self.repeat = repeat\n","           \n","    def answer(self, question):\n","        messages = [\n","        {\n","            \"role\": \"user\", \n","            \"content\": \n","        }\n","    ]\n","    \n","    query_prompt = tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=False\n","    )\n","    \n","        answers = [self.solve(question) for _ in range(self.repeat)]\n","        \n","    \n","    def solve(question):\n","        \n","        \n","if 'solve_problem' not in dir():\n","    def solve_problem(text):\n","        \n","        return 0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T07:54:00.793325Z","iopub.status.busy":"2024-05-19T07:54:00.792876Z","iopub.status.idle":"2024-05-19T07:54:00.802809Z","shell.execute_reply":"2024-05-19T07:54:00.801357Z","shell.execute_reply.started":"2024-05-19T07:54:00.793295Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","class FakeEnv:\n","    def __init__(self, questions):\n","        self.questions = questions\n","        self.answers = []\n","        \n","    def iter_test(self):\n","        for i, (q, a) in enumerate(self.questions):\n","            yield (\n","                pd.DataFrame([{'id':i, 'problem': q}]).set_index('id'), \n","                pd.DataFrame([{'id':i, 'answer':0, 'true answer': a}]).set_index('id')\n","            )\n","        \n","        \n","    def predict(self, submission):\n","        self.answers.append(submission)\n","        \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T07:55:51.341748Z","iopub.status.busy":"2024-05-19T07:55:51.341280Z","iopub.status.idle":"2024-05-19T07:55:51.348917Z","shell.execute_reply":"2024-05-19T07:55:51.347403Z","shell.execute_reply.started":"2024-05-19T07:55:51.341714Z"},"trusted":true},"outputs":[],"source":["fake_env = FakeEnv(\n","    [random_arithmetic_question() for _ in range(5)]+\n","    [random_equation_question() for _ in range(5)]\n",")\n","problems, submit_answer = fake_env.iter_test(), fake_env.predict "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T12:11:04.983061Z","iopub.status.busy":"2024-05-19T12:11:04.982685Z","iopub.status.idle":"2024-05-19T12:11:04.991748Z","shell.execute_reply":"2024-05-19T12:11:04.990828Z","shell.execute_reply.started":"2024-05-19T12:11:04.983029Z"},"trusted":true},"outputs":[],"source":["import random\n","\n","def random_arithmetic_question():\n","    while True:\n","        op = random.choice(\"+-*\")\n","        a, b = random.choices(range(1000), k=2)\n","        expression = f\"{a}{op}{b}\"\n","        result = eval(expression)\n","        if result >= 0:\n","            break\n","    answer = result % 1000\n","    if random.uniform(0, 2) > 1:\n","        expression.replace(\"*\", \"\\\\times\")\n","    return f\"What is ${expression}$?\", answer\n","\n","\n","def random_equation_question():\n","    while True:\n","        op = random.choice(\"+-*\")\n","        a, x = random.choices(range(100), k=2)\n","        expression = f\"{a}{op}{x}\"\n","        b = eval(expression)\n","        if b >= 0:\n","            break\n","    answer = x % 1000\n","    var_name = random.choice(\"abcxyz\")\n","    expression = f\"{a}{op}{var_name}={b}\"\n","    if random.uniform(0, 2) > 1:\n","        expression.replace(\"*\", \"\\\\times\")\n","    return f\"Solve ${expression}$ for ${var_name}$.\", answer\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T07:48:44.016777Z","iopub.status.busy":"2024-05-19T07:48:44.016058Z","iopub.status.idle":"2024-05-19T07:48:44.023521Z","shell.execute_reply":"2024-05-19T07:48:44.022352Z","shell.execute_reply.started":"2024-05-19T07:48:44.016740Z"},"trusted":true},"outputs":[],"source":["if do_test:\n","    print(random_arithmetic_question())\n","    print(random_equation_question())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8365361,"sourceId":73231,"sourceType":"competition"},{"datasetId":4281572,"sourceId":7369493,"sourceType":"datasetVersion"},{"datasetId":4720595,"sourceId":8012825,"sourceType":"datasetVersion"},{"datasetId":4728129,"sourceId":8023365,"sourceType":"datasetVersion"},{"modelInstanceId":3900,"sourceId":5112,"sourceType":"modelInstanceVersion"},{"modelInstanceId":4761,"sourceId":5994,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8318,"sourceId":11382,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8332,"sourceId":11394,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":724.728315,"end_time":"2024-02-29T09:37:08.760349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-29T09:25:04.032034","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"21267b653022419eb6fc3f47aa4db8ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926e7ccdad6440be85c76931860b744c","placeholder":"​","style":"IPY_MODEL_feef8334edb24f6da22e8bb1d8d80c67","value":"Loading checkpoint shards: 100%"}},"2144e851698b4707ad1c7fc29fe21b03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3963993becfa487c9ff725f211915e67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a725e1b0cc4ad78a62beab5f663065","placeholder":"​","style":"IPY_MODEL_fdb32baaed7145d8a8024b615ef242ca","value":" 19/19 [10:48&lt;00:00, 33.24s/it]"}},"5882b6e860be4a0db012a64fc0704a3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21267b653022419eb6fc3f47aa4db8ed","IPY_MODEL_d91eb83d016a4381828192a98f798f9b","IPY_MODEL_3963993becfa487c9ff725f211915e67"],"layout":"IPY_MODEL_6a892a5561f742bb9db9f13859c18e90"}},"6a892a5561f742bb9db9f13859c18e90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926e7ccdad6440be85c76931860b744c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91eb83d016a4381828192a98f798f9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2144e851698b4707ad1c7fc29fe21b03","max":19,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0693b32889c42b18b9a3844e045d048","value":19}},"e0693b32889c42b18b9a3844e045d048":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a725e1b0cc4ad78a62beab5f663065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb32baaed7145d8a8024b615ef242ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feef8334edb24f6da22e8bb1d8d80c67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
